import logging
import time
import requests
import json
import zipfile
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

from ..constants.api import EDINET_API_BASE_URL
from ..utils.fiscal_year import normalize_date_format, parse_date_string

logger = logging.getLogger(__name__)

class EdinetAPIClient:
    """EDINET API v2 ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, api_key: Optional[str] = None, cache_dir: Optional[str] = None):
        self.api_key = api_key
        self.base_url = EDINET_API_BASE_URL
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ç›¸å¯¾ãƒ‘ã‚¹æŒ‡å®šã‚’å»ƒæ­¢ã—ã€å¤–éƒ¨ã‹ã‚‰ã®æ³¨å…¥ãŒãªã„å ´åˆã¯
        # ä¸€æ™‚çš„ãªå ´æ‰€ã‹ã‚¨ãƒ©ãƒ¼ã¨ãªã‚‹ã‚ˆã†ã«èª˜å°ã—ã¾ã™ã€‚
        if cache_dir:
            self.cache_dir = Path(cache_dir)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ã‚¹ãƒ†ãƒ ã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¾ãŸã¯ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã® tmp_cache
            # (åŸºæœ¬çš„ã«ã¯ DataService ã‹ã‚‰ Application Support ã®ãƒ‘ã‚¹ãŒæ¸¡ã•ã‚Œã‚‹æƒ³å®š)
            self.cache_dir = Path("tmp_cache") / "edinet"
            
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–ï¼ˆã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³å†åˆ©ç”¨ã®ãŸã‚ï¼‰
        self.session = requests.Session()

    def update_api_key(self, api_key: str):
        """APIã‚­ãƒ¼ã‚’æ›´æ–°ã—ã¾ã™ã€‚"""
        self.api_key = api_key.strip() if api_key else ""

    def _request(self, endpoint: str, params: Dict[str, Any] = None, max_retries: int = 3) -> requests.Response:
        """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãAPIãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        if not self.api_key:
            raise ValueError("EDINET_API_KEY is not set")
            
        url = f"{self.base_url}{endpoint}"
        params = params or {}
        params["Subscription-Key"] = self.api_key
        
        last_exception = None
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    wait_time = 2 ** attempt
                    logger.warning(f"âš ï¸ [EDINET API] Retry attempt {attempt+1}/{max_retries} after {wait_time}s...")
                    time.sleep(wait_time)
                
                response = self.session.get(url, params=params, timeout=30)
                
                # Check JSON body for errors even if status_code is 200
                try:
                    data = response.json()
                    status_code_in_body = data.get("statusCode")
                    if status_code_in_body and status_code_in_body != 200:
                        message = data.get("message", "Unknown error")
                        logger.error(f"âŒ [EDINET API] Business logic error: {status_code_in_body} - {message}")
                        if status_code_in_body == 401:
                            raise ValueError(f"EDINET APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™: {message}")
                        raise requests.exceptions.HTTPError(f"EDINET API error: {status_code_in_body} - {message}", response=response)
                except ValueError as e:
                    # Not a JSON response or custom ValueError from above
                    if "EDINET APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™" in str(e):
                        raise
                    pass

                response.raise_for_status()
                return response
            except (requests.exceptions.RequestException, requests.exceptions.HTTPError, ValueError) as e:
                last_exception = e
                if isinstance(e, ValueError) and "EDINET APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™" in str(e):
                    raise
                
                status_code = getattr(e.response, 'status_code', None) if hasattr(e, 'response') else None
                if status_code in [429, 500, 502, 503, 504] or isinstance(e, requests.exceptions.ConnectionError):
                    continue
                else:
                    logger.error(f"âŒ [EDINET API] Non-retryable error: {e}")
                    raise
        
        logger.error(f"âŒ [EDINET API] All {max_retries} attempts failed. Last error: {last_exception}")
        raise last_exception

    def _get_search_cache_key(self, date_str: str) -> str:
        """æ¤œç´¢ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆï¼ˆæ—¥ä»˜ãƒ™ãƒ¼ã‚¹ï¼‰"""
        return f"search_{date_str}.json"

    def _load_search_cache(self, filename: str) -> Optional[List[Dict[str, Any]]]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ¤œç´¢çµæœã‚’ãƒ­ãƒ¼ãƒ‰"""
        cache_path = self.cache_dir / filename
        if cache_path.exists():
            try:
                with open(cache_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    return data
            except Exception as e:
                logger.warning(f"Cache load failed: {e}")
        return None

    def _save_search_cache(self, filename: str, data: List[Dict[str, Any]]):
        """æ¤œç´¢çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        cache_path = self.cache_dir / filename
        try:
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"Cache save failed: {e}")

    def search_documents(
        self,
        code: str,
        years: Optional[List[int]] = None,
        doc_type_code: Optional[str] = None,
        jquants_data: Optional[List[Dict[str, Any]]] = None,
        edinet_code: Optional[str] = None,
        max_documents: int = 2
    ) -> List[Dict[str, Any]]:
        """
        J-QUANTSã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦EDINETæ›¸é¡ã‚’æ¤œç´¢ï¼ˆæœŸé–“ãƒ™ãƒ¼ã‚¹ï¼‰
        
        æœ€é©åŒ–ï¼šæå‡ºæœŸé™ï¼ˆä¼šè¨ˆæœŸé–“çµ‚äº†ã‹ã‚‰90æ—¥/45æ—¥ï¼‰ã‹ã‚‰æ±ºç®—ç™ºè¡¨æ—¥ã¾ã§ã‚’ã€Œå¾Œæ–¹æ¢ç´¢ã€ã—ã€
        ã‹ã¤æ—¥ä»˜ã”ã¨ã®ãƒªã‚¹ãƒˆå–å¾—ã‚’ã€Œä¸¦åˆ—åŒ–ã€ã™ã‚‹ã“ã¨ã§é«˜é€ŸåŒ–ã—ã¾ã™ã€‚
        """
        if not self.api_key or not jquants_data:
            return []
            
        all_documents = []
        now = datetime.now()
        code_4digit = code[:4] if len(code) >= 4 else code
        
        # æœ€å¤§ä¸¦åˆ—æ•°ï¼ˆEDINET APIã®è² è·ã«é…æ…®ã—ã¦10ç¨‹åº¦ï¼‰
        MAX_WORKERS = 10
        
        # å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç›´æ¥å‡¦ç†
        for record in jquants_data:
            if len(all_documents) >= max_documents:
                logger.info(f"âœ… [EDINET] Found {max_documents} documents, stopping search early.")
                break
            
            fy_end = record.get("CurFYEn", "")
            per_en = record.get("CurPerEn", "")
            fy_st = record.get("CurFYSt", "")
            disc_date_str = record.get("DiscDate", "")
            period_type = record.get("CurPerType") or record.get("period_type", "FY")
            
            fiscal_year = record.get("fiscal_year")
            if not fiscal_year and fy_st:
                fy_st_date = parse_date_string(fy_st)
                if fy_st_date: fiscal_year = fy_st_date.year
            
            target_period_end_str = per_en if period_type != "FY" and per_en else fy_end
            period_end_date = parse_date_string(target_period_end_str)
            disc_date_formatted = normalize_date_format(disc_date_str)
            
            if not period_end_date or not disc_date_formatted:
                continue
                
            try:
                disc_date_obj = datetime.strptime(disc_date_formatted, "%Y-%m-%d")
                if disc_date_obj > now: continue
                
                # æ¤œç´¢ç¯„å›²ã®è¨­å®š:
                # é–‹å§‹: æ±ºç®—çŸ­ä¿¡ã®é–‹ç¤ºæ—¥(DiscDate)
                # çµ‚äº†: min(ä¼šè¨ˆæœŸé–“çµ‚äº†æ—¥ + 97æ—¥, ç¾åœ¨æ—¥æ™‚)
                search_start = disc_date_obj
                search_end = min(period_end_date + timedelta(days=97), now)
                
                target_doc_types = [doc_type_code] if doc_type_code else (
                    ["120"] if period_type == "FY" else (
                        ["140", "160"] if period_type in ["2Q", "Q2"] else []
                    )
                )
                
                if not target_doc_types: continue
                
                logger.info(f"ğŸ” [EDINET] Searching period={period_type}, fiscal_year={fiscal_year}, target={search_end.strftime('%Y-%m-%d')} back to {search_start.strftime('%Y-%m-%d')}")
                
                # æ¤œç´¢å¯¾è±¡ã®æ—¥ä»˜ãƒªã‚¹ãƒˆã‚’ã€Œæ–°ã—ã„é †ï¼ˆå¾Œæ–¹ï¼‰ã€ã«ç”Ÿæˆ
                target_dates = []
                curr = search_end
                while curr >= search_start:
                    target_dates.append(curr.strftime("%Y-%m-%d"))
                    curr -= timedelta(days=1)
                
                found_for_this_record = False
                
                # ä¸¦åˆ—ã§æ—¥ä»˜ä¸€è¦§ã‚’å–å¾—ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
                # 1ã¤è¦‹ã¤ã‹ã£ãŸã‚‰ãã“ã§ä¸­æ–­ã—ãŸã„ãŒã€ä¸¦åˆ—å®Ÿè¡Œä¸­ã¯ãƒãƒƒãƒå˜ä½ã§å‡¦ç†
                BATCH_SIZE = MAX_WORKERS
                for i in range(0, len(target_dates), BATCH_SIZE):
                    batch = target_dates[i:i+BATCH_SIZE]
                    
                    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                        future_to_date = {executor.submit(self._get_documents_for_date, d): d for d in batch}
                        
                        # å®Œäº†ã—ãŸé †ã«ãƒã‚§ãƒƒã‚¯ï¼ˆãŸã ã—æ—¥ä»˜ã®æ–°ã—ã„é †ã«å‡¦ç†ã—ãŸã„å ´åˆã¯å·¥å¤«ãŒå¿…è¦ï¼‰
                        # ã“ã“ã§ã¯å®Œäº†é †ã§ã¯ãªãã€batchå†…ã§ã®é †åºã‚’ç¶­æŒã—ã¦ãƒã‚§ãƒƒã‚¯ã™ã‚‹
                        results_map = {}
                        for future in as_completed(future_to_date):
                            date_str = future_to_date[future]
                            try:
                                results_map[date_str] = future.result()
                            except Exception as e:
                                logger.error(f"Error fetching docs for {date_str}: {e}")
                                results_map[date_str] = []
                        
                        # ãƒãƒƒãƒå†…ã®æ—¥ä»˜ã‚’ã€Œæ–°ã—ã„é †ã€ã«èµ°æŸ»ã—ã¦ãƒãƒƒãƒãƒ³ã‚°
                        for date_str in batch:
                            documents = results_map.get(date_str, [])
                            for doc in documents:
                                current_edinet_code = doc.get("edinetCode")
                                sec_code = str(doc.get("secCode", "")).strip()
                                
                                is_match = False
                                if edinet_code and current_edinet_code == edinet_code:
                                    is_match = True
                                elif sec_code.startswith(code_4digit):
                                    is_match = True
                                    
                                if is_match:
                                    dt = doc.get("docTypeCode", "")
                                    if not target_doc_types or dt in target_doc_types:
                                        desc = doc.get("docDescription", "")
                                        if desc and ("è¨‚æ­£" in desc or "è£œæ­£" in desc): continue
                                        
                                        logger.info(f"âœ¨ [EDINET HIT] {sec_code}: {desc} ({date_str}) ID={doc.get('docID')}")
                                        doc["fiscal_year"] = fiscal_year
                                        doc["jquants_fy_end"] = fy_end
                                        doc["period_type"] = period_type
                                        all_documents.append(doc)
                                        found_for_this_record = True
                                        break
                            if found_for_this_record: break
                    if found_for_this_record: break
                        
            except Exception as e:
                logger.error(f"âŒ [EDINET] Error processing record: {e}", exc_info=True)
                    
        # é‡è¤‡é™¤å» (docID)
        seen_ids = set()
        unique_docs = []
        for d in all_documents:
            if d["docID"] not in seen_ids:
                seen_ids.add(d["docID"])
                unique_docs.append(d)
        
        return unique_docs

    def _get_documents_for_date(self, date_str: str) -> List[Dict[str, Any]]:
        """ç‰¹å®šã®æ—¥ä»˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰"""
        cache_key = self._get_search_cache_key(date_str)
        documents = self._load_search_cache(cache_key)
        if documents is not None:
            return documents
        
        try:
            response = self._request("/documents.json", {"date": date_str, "type": 2})
            data = response.json()
            documents = data.get("results", [])
            self._save_search_cache(cache_key, documents)
            return documents
        except Exception:
            return []

    def download_document(self, doc_id: str, doc_type: int = 1, save_dir: Optional[Path] = None) -> Optional[Path]:
        """æ›¸é¡ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ1=XBRLã®ã¿ç¶­æŒã€‚æ—§2=PDFã¯å»ƒæ­¢ï¼‰"""
        if doc_type != 1:
            logger.warning(f"âš ï¸ [EDINET] ID={doc_id} ã® PDF ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯å»ƒæ­¢ã•ã‚Œã¾ã—ãŸã€‚")
            return None

        if save_dir is None: save_dir = self.cache_dir
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        dest = save_dir / f"{doc_id}_xbrl"
        if dest.exists() and dest.is_dir(): return dest
            
        if not self.api_key: return None
        
        try:
            response = self._request(f"/documents/{doc_id}", {"type": 1})
            zip_path = save_dir / f"{doc_id}.zip"
            with open(zip_path, "wb") as f: f.write(response.content)
            dest.mkdir(parents=True, exist_ok=True)
            with zipfile.ZipFile(zip_path, "r") as z: z.extractall(dest)
            zip_path.unlink()
            return dest
        except Exception as e:
            logger.error(f"âŒ [EDINET] XBRL Download error {doc_id}: {e}")
            return None


    def search_recent_reports(
        self,
        code: str,
        jquants_data: List[Dict[str, Any]],
        max_years: int = 5,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’5å¹´ã«æ‹¡å¤§
        doc_types: Optional[List[str]] = None,
        max_documents: int = 10
    ) -> List[Dict[str, Any]]:
        """
        æœ€æ–°ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€ç›´è¿‘Nå¹´åˆ†ã®å ±å‘Šæ›¸ã‚’è‡ªå‹•æ¤œç´¢
        """
        if not jquants_data:
            return []
            
        from ..utils.jquants_utils import prepare_edinet_search_data
        
        # å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨ã—ã¦å¹´åº¦ãƒ»å››åŠæœŸã”ã¨ã«ä»£è¡¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        # max_records ã¯å¤šã‚ã«æŒ‡å®šã—ã¦ãŠãï¼ˆmax_years * 2 ã§ååˆ†ã ãŒã€ä½™è£•ã‚’æŒãŸã›ã‚‹ï¼‰
        annual_data_idx, years_list = prepare_edinet_search_data(
            jquants_data, 
            max_records=max_years * 3 
        )
        
        # è¢«ã£ã¦ã„ã‚‹å¹´åº¦ã®æ›¸é¡ã‚’æ¼ã‚Œãªãå–å¾—ã§ãã‚‹ã‚ˆã†ã€ã‚¹ãƒ©ã‚¤ã‚¹ã¯è¡Œã‚ãš
        # prepare_edinet_search_data ã§å–å¾—ã•ã‚ŒãŸå¹´åº¦ãƒªã‚¹ãƒˆï¼ˆæ—¢ã« max_records ã§åˆ¶é™æ¸ˆã¿ï¼‰ã‚’ãã®ã¾ã¾ä½¿ã†
        years = years_list
        
        # å®Ÿéš›ã®æ¤œç´¢ã«ä½¿ç”¨ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚‚ã€é¸æŠã•ã‚ŒãŸå¹´åº¦ã«å«ã¾ã‚Œã‚‹ã‚‚ã®ã«é™å®š
        recent_data = [
            d for d in annual_data_idx 
            if d.get("fiscal_year") in years
        ]
        
        return self.search_documents(
            code=code,
            years=years,
            jquants_data=recent_data,
            doc_type_code=doc_types[0] if doc_types and len(doc_types) == 1 else None,
            max_documents=max_documents
        )

    def fetch_latest_annual_report(self, code: str, jquants_data: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """æœ€æ–°ã®æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸(120)ã‚’1ä»¶å–å¾—"""
        docs = self.search_recent_reports(code, jquants_data, max_years=10, doc_types=["120"])
        annual_reports = [d for d in docs if d.get("docTypeCode") == "120"]
        if annual_reports:
            return sorted(annual_reports, key=lambda x: x.get("submitDateTime", ""), reverse=True)[0]
        return None
